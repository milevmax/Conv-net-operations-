{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951db6f3-8f1b-4baa-907b-f0f256b51446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063bc08d-1f8a-4aa0-8ac3-5bd07fe31408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = ''\n",
    "\n",
    "imgset = torchvision.datasets.MNIST(root=data_dir, train=True, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4bd4b32-d76c-45e4-9aa5-cbd540fe1ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: \n",
       "    Split: Train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d9ada5-dfeb-42f5-855e-4b2366eec59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = imgset[0]\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.title(label)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8294e6b2-2369-4ae7-8c90-8f9f63891796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = ToTensor()\n",
    "image_tensor_dev = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6753079e-b677-4b58-ab5f-9aafbbef74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv2d(1, 1, 3)\n",
    "output = conv_layer(image_tensor_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bef6ccb-5b69-4e6e-b721-ed5b78e233ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "t4d = torch.empty(100, 32, 10, 10)\n",
    "p1d = (1, 1, 1, 1) # pad last dim by 1 on each side\n",
    "out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ccbf817-6ef7-4344-926f-b88ada376af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image.\n",
    "    \n",
    "    Argument:\n",
    "    X -- torch tensor of shape (m, n_C, n_H, n_W) representing a batch of m images.\n",
    "        n_C, n_H, n_W denote respectively the number of channels, height and width \n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_C, n_H + 2 * pad, n_W + 2 * pad)\n",
    "    \"\"\"\n",
    "    pad_per_dim = (pad, pad, pad, pad)\n",
    "    X_pad = F.pad(X, pad_per_dim, \"constant\", 0) \n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d157a846-d576-4387-be48-0613840db12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "out = zero_pad(t4d, 1)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a78a89a-0616-45cb-909e-a99b034c235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev).\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameter contained in a window - scalar value\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "    \n",
    "    s = torch.mul(a_slice_prev, W)\n",
    "    Z = torch.sum(s) + b\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f943c583-b79c-4cfb-9337-60e2fa04a93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22., 10.],\n",
       "        [ 1., -7.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1., -1.], [1., -1.]])\n",
    "b = torch.tensor([[22., -10.], [1., 7.]])\n",
    "torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4736b79a-3918-47c8-a945-883ff528eebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.5010)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_slice_prev = torch.tensor([[10, 9], [0, -1]])\n",
    "W = torch.tensor([[0.5, 0.5], [0.1, -2]])\n",
    "b = .001\n",
    "conv_single_step(a_slice_prev, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d63ee5b2-c82d-4fb9-8f3b-4a5923e82a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(34.5010)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_slice_prev = torch.stack([a_slice_prev] * 3)\n",
    "W = torch.stack([W] * 3)\n",
    "b = .001\n",
    "conv_single_step(a_slice_prev, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7f80f4f-6751-4653-8e82-0e3f90dd31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- input for current conv layer, torch tensor of shape (m, n_C, n_H, n_W)\n",
    "    W -- Weights, torch tensor of shape (n_C, n_C_prev, f, f)\n",
    "    b -- Biases, torch tensor of shape (n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, torch tensor of shape (m, n_C, n_H, n_W)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
    "    (n_C, n_C_prev, f, f) = W.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume\n",
    "    n_H = int((n_H_prev+(2*pad)-f)/stride)+1\n",
    "    n_W = int((n_W_prev+(2*pad)-f)/stride)+1\n",
    "    \n",
    "    Z = torch.zeros(m, n_C, n_H, n_W)\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]      \n",
    "        for h in range(n_H):\n",
    "            vert_start = stride * h\n",
    "            vert_end = vert_start + f\n",
    "            \n",
    "            for w in range(n_W):\n",
    "                horiz_start = stride * w\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range(n_C):                                       \n",
    "                    a_slice_prev = a_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end]\n",
    "                    \n",
    "                    weights = W[c, :, :, :]\n",
    "                    biases  = b[c]\n",
    "                    Z[i, c, h, w] = conv_single_step(a_slice_prev, weights, biases)\n",
    "        \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5798c5a-03b8-4c8b-89ce-adfbf2836e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev = torch.randn(1, 4, 10, 10, requires_grad=True)\n",
    "W = torch.randn(8, 4, 3, 3, requires_grad=True)\n",
    "b = torch.randn(8, requires_grad=True)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache = conv_forward(A_prev, W, b, hparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0254010-b950-4a71-8f21-289a312cd6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 5, 5])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7695d32f-b636-4af6-953c-4ad3ec08c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_weights = W\n",
    "custom_bias = b\n",
    "\n",
    "class CustomConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(CustomConv2d, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=2, padding=1, bias=True)\n",
    "        \n",
    "        self.conv.weight = nn.Parameter(custom_weights)\n",
    "        self.conv.bias = nn.Parameter(custom_bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "68f8f17a-b6f8-4b36-b159-58c939876746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_conv_layer = CustomConv2d(4, 8, 3)\n",
    "input_tensor = A_prev\n",
    "output = custom_conv_layer(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5a306bd-d907-45f3-82e8-feef83cc7190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 5, 5])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2efe6e14-0a9a-46c4-939c-4a71a3f23161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 3, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_conv_layer.conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e22d3cc-2d57-444e-9b73-3a445fd5c82a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 3, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbad9a3a-9593-48b3-9053-116f3066ee58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_conv_layer.conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d75e35c-6da9-4e26-91b8-f44ecae8b5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "599a2a8b-a374-4610-99f9-ace5d8fe9768",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.0180, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[0,0,4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31888956-2254-4433-b67d-05ba524ba3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.0180, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0, 0, 4, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d607ad-8173-46a2-ac47-a7ecb72708f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e7ff5d5-fa74-42c3-b818-0b74410936a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- input data, torch tensor of shape (m, n_C_prev, n_H_prev, n_W_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a torch tensor of shape (m, n_C, n_H, n_W)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
    "    \n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = torch.zeros(m, n_C, n_H, n_W)              \n",
    "\n",
    "    for i in range(m):               \n",
    "        a_prev_slice = A_prev[i]          \n",
    "        for h in range(n_H):\n",
    "            vert_start = stride * h \n",
    "            vert_end = vert_start  + f\n",
    "            \n",
    "            for w in range(n_W):      \n",
    "                horiz_start = stride * w\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range(n_C):                                           \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad\n",
    "                    a_slice_prev = a_prev_slice[c, vert_start:vert_end,horiz_start:horiz_end]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i, c, h, w] = torch.max(a_slice_prev)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, c, h, w] = torch.mean(a_slice_prev)\n",
    "                    else:\n",
    "                        print(mode+ \"-type pooling layer NOT Defined\") \n",
    "        \n",
    "    # Store the input and hparameters in \"cache\" for backprop\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5542896-b987-4946-90d6-5e774ecff027",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparameters = {\"stride\" : 1, \"f\": 3}\n",
    "A, cache = pool_forward(A_prev, hparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5a7700d-1c14-44b2-8285-467417922885",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.6702, 1.9331, 1.9331, 1.9331, 1.0379, 0.9550, 0.9550, 1.4922],\n",
       "          [1.6702, 1.9331, 1.9331, 1.9331, 0.6266, 0.9550, 0.9550, 1.4922],\n",
       "          [0.9391, 1.9331, 1.9331, 1.9331, 1.6324, 1.6324, 1.6324, 1.3696],\n",
       "          [0.9391, 0.9872, 1.1150, 1.1150, 1.6324, 1.6324, 1.6324, 0.8420],\n",
       "          [0.9391, 0.9872, 1.3281, 1.3281, 1.6324, 1.6324, 1.6324, 0.8420],\n",
       "          [0.6888, 0.8173, 1.3281, 1.7443, 1.7443, 1.7443, 1.2384, 0.8420],\n",
       "          [0.9590, 0.9590, 1.3281, 1.7443, 1.7443, 1.7443, 1.2384, 1.3729],\n",
       "          [0.9590, 1.0856, 1.0856, 1.7443, 1.7443, 1.7443, 1.1965, 1.3729]],\n",
       "\n",
       "         [[1.4411, 1.4411, 1.7790, 1.7790, 1.7790, 1.5395, 2.0128, 2.0128],\n",
       "          [0.6728, 0.8766, 1.7790, 1.7790, 1.7790, 1.5395, 2.0128, 2.0128],\n",
       "          [0.9204, 1.3869, 1.7790, 1.7790, 1.7790, 1.5395, 1.5395, 1.0171],\n",
       "          [0.9204, 1.3869, 1.4085, 1.4085, 1.4085, 1.0986, 1.6139, 1.6139],\n",
       "          [2.4228, 2.4228, 1.4085, 1.4085, 1.4085, 1.0986, 1.6139, 1.6139],\n",
       "          [2.4228, 2.4228, 1.4085, 1.4085, 1.4085, 1.0986, 1.6139, 1.6139],\n",
       "          [2.4228, 2.4228, 0.6047, 2.1470, 2.1470, 2.1470, 0.9545, 1.6200],\n",
       "          [1.3501, 1.3501, 0.6047, 2.1470, 2.1470, 2.1470, 0.9545, 1.6200]],\n",
       "\n",
       "         [[1.4524, 0.7104, 0.8101, 1.7906, 1.7906, 1.7906, 1.2201, 1.2201],\n",
       "          [1.1498, 0.7104, 0.8101, 1.7906, 1.7906, 1.7906, 1.7284, 1.7284],\n",
       "          [1.5270, 1.5270, 1.5270, 0.1835, 0.1835, 1.7284, 1.7284, 1.7284],\n",
       "          [1.5270, 1.5270, 1.5270, 1.2902, 1.2902, 1.7284, 1.7284, 1.7284],\n",
       "          [1.7096, 1.7096, 1.6224, 1.6224, 1.6224, 1.2902, 1.0431, 1.0431],\n",
       "          [1.7096, 1.7096, 1.6224, 1.6224, 1.6224, 1.2902, 0.8841, 0.8841],\n",
       "          [1.7096, 1.7096, 1.6224, 1.6224, 2.3345, 2.3345, 2.3345, 1.7757],\n",
       "          [1.9719, 1.9719, 1.1155, 1.1155, 2.3345, 2.3345, 2.3345, 1.7757]],\n",
       "\n",
       "         [[2.2324, 2.2324, 2.2324, 2.0924, 2.0924, 0.9265, 0.9265, 2.1664],\n",
       "          [2.2324, 2.2324, 2.2324, 2.0924, 2.0924, 2.6484, 2.6484, 2.6484],\n",
       "          [2.1675, 2.1675, 2.0924, 2.0924, 2.0924, 2.6484, 2.6484, 2.6484],\n",
       "          [1.4567, 1.4567, 1.1371, 1.1371, 1.6672, 2.6484, 2.6484, 2.6484],\n",
       "          [1.4567, 1.5671, 1.5671, 1.5671, 1.3604, 1.3604, 1.3604, 1.1598],\n",
       "          [1.4567, 1.5671, 1.5671, 1.5671, 1.1371, 0.4811, 0.6059, 1.1598],\n",
       "          [0.6552, 1.5671, 1.5671, 1.5671, 0.7478, 0.4732, 0.4732, 0.4732],\n",
       "          [0.4230, 1.3904, 1.8056, 1.8056, 1.8056, 0.4732, 1.2006, 1.2006]]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e26cc2a0-66d6-4e01-8a67-3852fd340745",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool = nn.MaxPool2d(3, stride=1)\n",
    "output = maxpool(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "338c3cd5-a5a2-42d9-a134-e099e81a53da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 8])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4287861-75f3-439b-8aff-5841add877ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 8])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12ccbf85-bbf8-4f53-bd19-b1ef46ec9dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5270, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0, 2, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6baa295b-4035-42a4-ab93-446c82f2b605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5270, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0, 2, 2, 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
